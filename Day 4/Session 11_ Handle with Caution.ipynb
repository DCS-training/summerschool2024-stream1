{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CDCS Summer School\n",
    "# A Gentle Introduction to Coding for Data Analysis\n",
    "## Session 11: Handle with Caution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------\n",
    "\n",
    "### Learning objectives for this session:\n",
    "\n",
    "\n",
    "At the end of this notebook you will know:\n",
    "\n",
    "1. What the pandas library is.\n",
    "2. Basic understanding of the Palmer Penguins dataset.\n",
    "3. How to load data using pandas.\n",
    "4. What a dataframe structure is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------\n",
    "\n",
    "## 1. Pandas? I thought we were on about Penguins?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are absolutely correct if you had the question above. For some reason (unknown to me but there probably is something online about it), a lot of Python related packages and features have animal related names. Pandas is an open-source data analysis and manipulation tool built on top of the Python programming language. It is widely used in data science, machine learning, and artificial intelligence due to its robust data structures and powerful data manipulation capabilities.\n",
    "\n",
    "Key features of pandas include:\n",
    "\n",
    "- **DataFrames**: A 2-dimensional labeled data structure with columns of potentially different types.\n",
    "- **Series**: A 1-dimensional labeled array capable of holding any data type.\n",
    "- **Indexing**: Easy and flexible selection and manipulation of data.\n",
    "- **Data alignment**: Automatic and explicit data alignment for managing missing data.\n",
    "- **Group by**: Splitting data into groups based on some criteria.\n",
    "- **Data wrangling**: Combining, merging, and reshaping data sets.\n",
    "- **I/O tools**: Tools for loading data from various file formats and databases.\n",
    "\n",
    "Like other packages we have seen we have to import the package in each notebook when we want to use it. The convention with `pandas` is to import it with the 'nickname' `pd`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the package.\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we start to explore the insides of the pandas package. It's not 100% necessary to understand every single one of the functions within the package, but in your pair try to get a general understanding of the different functions that are now at your disposal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ArrowDtype\n",
      "BooleanDtype\n",
      "Categorical\n",
      "CategoricalDtype\n",
      "CategoricalIndex\n",
      "DataFrame\n",
      "DateOffset\n",
      "DatetimeIndex\n",
      "DatetimeTZDtype\n",
      "ExcelFile\n",
      "ExcelWriter\n",
      "Flags\n",
      "Float32Dtype\n",
      "Float64Dtype\n",
      "Grouper\n",
      "HDFStore\n",
      "Index\n",
      "IndexSlice\n",
      "Int16Dtype\n",
      "Int32Dtype\n",
      "Int64Dtype\n",
      "Int8Dtype\n",
      "Interval\n",
      "IntervalDtype\n",
      "IntervalIndex\n",
      "MultiIndex\n",
      "NA\n",
      "NaT\n",
      "NamedAgg\n",
      "Period\n",
      "PeriodDtype\n",
      "PeriodIndex\n",
      "RangeIndex\n",
      "Series\n",
      "SparseDtype\n",
      "StringDtype\n",
      "Timedelta\n",
      "TimedeltaIndex\n",
      "Timestamp\n",
      "UInt16Dtype\n",
      "UInt32Dtype\n",
      "UInt64Dtype\n",
      "UInt8Dtype\n",
      "__all__\n",
      "__builtins__\n",
      "__cached__\n",
      "__doc__\n",
      "__docformat__\n",
      "__file__\n",
      "__git_version__\n",
      "__loader__\n",
      "__name__\n",
      "__package__\n",
      "__path__\n",
      "__spec__\n",
      "__version__\n",
      "_built_with_meson\n",
      "_config\n",
      "_is_numpy_dev\n",
      "_libs\n",
      "_pandas_datetime_CAPI\n",
      "_pandas_parser_CAPI\n",
      "_testing\n",
      "_typing\n",
      "_version_meson\n",
      "annotations\n",
      "api\n",
      "array\n",
      "arrays\n",
      "bdate_range\n",
      "compat\n",
      "concat\n",
      "core\n",
      "crosstab\n",
      "cut\n",
      "date_range\n",
      "describe_option\n",
      "errors\n",
      "eval\n",
      "factorize\n",
      "from_dummies\n",
      "get_dummies\n",
      "get_option\n",
      "infer_freq\n",
      "interval_range\n",
      "io\n",
      "isna\n",
      "isnull\n",
      "json_normalize\n",
      "lreshape\n",
      "melt\n",
      "merge\n",
      "merge_asof\n",
      "merge_ordered\n",
      "notna\n",
      "notnull\n",
      "offsets\n",
      "option_context\n",
      "options\n",
      "pandas\n",
      "period_range\n",
      "pivot\n",
      "pivot_table\n",
      "plotting\n",
      "qcut\n",
      "read_clipboard\n",
      "read_csv\n",
      "read_excel\n",
      "read_feather\n",
      "read_fwf\n",
      "read_gbq\n",
      "read_hdf\n",
      "read_html\n",
      "read_json\n",
      "read_orc\n",
      "read_parquet\n",
      "read_pickle\n",
      "read_sas\n",
      "read_spss\n",
      "read_sql\n",
      "read_sql_query\n",
      "read_sql_table\n",
      "read_stata\n",
      "read_table\n",
      "read_xml\n",
      "reset_option\n",
      "set_eng_float_format\n",
      "set_option\n",
      "show_versions\n",
      "test\n",
      "testing\n",
      "timedelta_range\n",
      "to_datetime\n",
      "to_numeric\n",
      "to_pickle\n",
      "to_timedelta\n",
      "tseries\n",
      "unique\n",
      "util\n",
      "value_counts\n",
      "wide_to_long\n"
     ]
    }
   ],
   "source": [
    "# Here we make a function which looks inside the package 'pd' ie. pandas, and then \n",
    "# prints all the different things that pandas can do.\n",
    "def PandasFunctionInside():\n",
    "    for option in dir(pd):\n",
    "        print(option)\n",
    "\n",
    "# Call the function to look.\n",
    "PandasFunctionInside()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "## 2. The Palmer Penguins Dataset\n",
    "\n",
    "As has been alluded to throughout the week so far, we will be using the Palmer Penguins dataset today and tomorrow to look at using data in practice with everything we have done so far. \n",
    "\n",
    "Of course we are going to learn how to do a lot of things with the data, but a key step when using programming and data is to, before even loading in the data to your Python console, is to have a good understanding of your data already. Therefore we are going to take some time to understand what the Palmer Penguins data is.\n",
    "\n",
    "The Palmer Penguins dataset was collected as part of research conducted by Dr. Kristen Gorman and colleagues at the Palmer Station, a United States research station located on Anvers Island in Antarctica. The data collection was part of the Long Term Ecological Research (LTER) program, which aims to study the long-term effects of climate change and ecological processes in the region.The primary goal of the research was to understand the foraging ecology and reproductive success of the three penguin species. The measurements were taken during the breeding seasons to monitor changes in the physical characteristics and population dynamics of the penguins.\n",
    "\n",
    "Key publications related to the Palmer Penguins dataset include:\n",
    "\n",
    "Gorman, K. B., Williams, T. D., &amp; Fraser, W. R. (2014). Ecological sexual dimorphism and environmental variability within a community of Antarctic penguins (genus Pygoscelis). PLOS ONE, 9(3), e90081.\n",
    "- This paper explores the ecological differences between male and female penguins and how environmental variability affects these differences.\n",
    "\n",
    "Gorman, K. B., Williams, T. D., &amp; Fraser, W. R. (2010). Post-fledging survival of Adélie penguins at Palmer Station, Antarctica. Marine Ecology Progress Series, 405, 273-285.\n",
    "- This paper focuses on the survival rates of Adélie penguins after they leave the nest and how various factors influence their chances of survival."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detailed Description of Each Feature\n",
    "\n",
    "1. Species\n",
    "\n",
    "The dataset includes three species of penguins: Adelie, Chinstrap, and Gentoo. Each species has distinct physical characteristics and behaviors.\n",
    "\n",
    "\n",
    "2. Island\n",
    "\n",
    "Penguins in the dataset were observed on three different islands: Biscoe, Dream, and Torgersen. These islands are part of the Palmer Archipelago in Antarctica.\n",
    "\n",
    "\n",
    "3. Bill Length (mm)\n",
    "\n",
    "This feature measures the length of a penguin's bill in millimeters. Bill length can vary significantly between species and is an important characteristic for identifying species.\n",
    "\n",
    "\n",
    "4. Bill Depth (mm)\n",
    "\n",
    "This feature measures the depth of a penguin's bill in millimeters. Bill depth, like bill length, is an important characteristic for species identification.\n",
    "\n",
    "\n",
    "5. Flipper Length (mm)\n",
    "\n",
    "This feature measures the length of a penguin's flipper in millimeters. Flipper length is related to a penguin's swimming ability and varies between species.\n",
    "\n",
    "\n",
    "6. Body Mass (g)\n",
    "\n",
    "This feature measures the body mass of a penguin in grams. Body mass can provide insights into the health and nutrition of the penguins.\n",
    "\n",
    "\n",
    "7. Sex\n",
    "\n",
    "This feature indicates the sex of the penguin, which can be either male or female. Understanding the sex distribution in the dataset can help in studying gender-related differences.\n",
    "\n",
    "\n",
    "8. Year\n",
    "\n",
    "This feature indicates the year when the observation was made. Analyzing data over multiple years can help identify trends and changes in penguin populations over time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "## 3. Loading in the data with and without pandas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you will find with most programming languages, there are a multitude of ways in which you can load in data. Luckily pandas makes this quite streamlined. We will next explore the different ways that this can be done.\n",
    "\n",
    "Loading data efficiently is a critical step in any data analysis process. While Python's built-in functions can handle basic data loading tasks, the pandas library offers a more powerful and flexible approach. In this section, we will explore different ways to load data using both built-in functions and pandas, highlighting the advantages of using pandas.\n",
    "\n",
    "Python's built-in functions such as open, read, and csv.reader can be used to load data from files. Let's see how we can load a CSV file using these functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using built-in functions to load a CSV file\n",
    "import csv\n",
    "\n",
    "file_path = 'data/palmer_penguins.csv'\n",
    "\n",
    "# Method 1: Using csv.reader\n",
    "with open(file_path, mode='r') as file:\n",
    "    csv_reader = csv.reader(file)\n",
    "    data = [row for row in csv_reader]\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "for row in data[:5]:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas provides a much simpler and more efficient way to load data from various sources. Here, we will demonstrate how to load a CSV file using the pd.read_csv function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using pandas to load a CSV file\n",
    "import pandas as pd\n",
    "\n",
    "file_path = 'data/palmer_penguins.csv'\n",
    "\n",
    "# Method 2: Using pd.read_csv\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison of Methods\n",
    "\n",
    "Let's compare the two methods in terms of simplicity, readability, and functionality.\n",
    "\n",
    "- Simplicity: The pandas method is more concise and requires fewer lines of code.\n",
    "- Readability: The pandas method is easier to read and understand, especially for large datasets.\n",
    "- Functionality: Pandas provides additional features such as handling missing values, parsing dates, and setting column names.\n",
    "\n",
    "### Handling Different Data Formats\n",
    "\n",
    "Pandas can handle various data formats such as CSV, Excel, JSON, and SQL databases. Let's look at how to load data from these different formats using pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading an Excel file using pandas\n",
    "file_path = 'data/palmer_penguins.xlsx'\n",
    "\n",
    "# Method 3: Using pd.read_excel\n",
    "data = pd.read_excel(file_path)\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading a JSON file using pandas\n",
    "file_path = 'data/palmer_penguins.json'\n",
    "\n",
    "# Method 4: Using pd.read_json\n",
    "data = pd.read_json(file_path, lines=True)\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading data from a SQL database using pandas\n",
    "import sqlite3\n",
    "\n",
    "db_path = 'data/palmer_penguins.db'\n",
    "query = 'SELECT * FROM penguins'\n",
    "\n",
    "# Method 5: Using pd.read_sql_query\n",
    "conn = sqlite3.connect(db_path)\n",
    "data = pd.read_sql_query(query, conn)\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error Handling in Data Loading\n",
    "\n",
    "Pandas provides robust error handling mechanisms to deal with issues during data loading, such as missing files or incorrect formats. It may be that you need to combine your existing knowledge of loops to do this. An additional type of loop we covered in session 10, is a try/except loops to help deal with errors. These can sometimes be a little fiddly --  definetly look more at this after the summer school."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error handling during data loading with pandas\n",
    "file_path = 'path/to/your/nonexistent_file.csv'\n",
    "\n",
    "# Method 6: Using try-except block\n",
    "try:\n",
    "    data = pd.read_csv(file_path)\n",
    "    print(data.head())\n",
    "except FileNotFoundError:\n",
    "    print(f\"The file at {file_path} was not found.\")\n",
    "except pd.errors.ParserError:\n",
    "    print(f\"There was a parsing error while reading the file at {file_path}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "## 4. What is a dataframe structure?\n",
    "\n",
    "A pandas DataFrame is a two-dimensional, size-mutable, and potentially heterogeneous tabular data structure with labeled axes (ie it has rows and columns). It's similar to a spreadsheet or SQL table and is one of the most commonly used data structures for data manipulation in pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'data/palmer_penguins.csv'\n",
    "penguins = pd.read_csv(file_path)\n",
    "penguins.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each column in a DataFrame has a specific data type. You can view the data types of all columns using the dtypes attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Viewing data types of each column\n",
    "penguins.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use various methods to explore the structure of a DataFrame, such as shape, columns, and index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Viewing the shape of the DataFrame (number of rows and columns)\n",
    "penguins.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Viewing the column names\n",
    "penguins.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Viewing the index (row labels)\n",
    "penguins.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The info method provides a concise summary of the DataFrame, including the number of non-null entries and data types of each column. We will be looking much more in the next session about how to get more detailed summaries and overviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Viewing concise summary of the DataFrame\n",
    "penguins.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a DataFrame from Scratch\n",
    "\n",
    "Let's create a small DataFrame from scratch with some penguin data to illustrate how to combine data structures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a small DataFrame from scratch\n",
    "data = {\n",
    "    'species': ['Adelie', 'Chinstrap', 'Gentoo'],\n",
    "    'island': ['Torgersen', 'Dream', 'Biscoe'],\n",
    "    'bill_length_mm': [39.1, 48.7, 50.0],\n",
    "    'bill_depth_mm': [18.7, 17.4, 15.3],\n",
    "    'flipper_length_mm': [181, 195, 210],\n",
    "    'body_mass_g': [3750, 3800, 5000],\n",
    "    'sex': ['male', 'female', 'male'],\n",
    "    'year': [2007, 2008, 2009]\n",
    "}\n",
    "\n",
    "new_penguins = pd.DataFrame(data)\n",
    "new_penguins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenating DataFrames\n",
    "combined_penguins = pd.concat([penguins, new_penguins], ignore_index=True)\n",
    "combined_penguins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Viewing data types of the combined DataFrame\n",
    "combined_penguins.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing the data type of the 'year' column to string\n",
    "combined_penguins['year'] = combined_penguins['year'].astype(str)\n",
    "combined_penguins.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accessing Specific Columns in a DataFrame\n",
    "Understanding how to access and manipulate specific columns in a DataFrame is crucial for data analysis. Here, we will cover various methods to select and work with columns in a DataFrame.\n",
    "\n",
    "You can select a single column by using the column name as a key. This will return a Series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting a single column\n",
    "species = penguins['species']\n",
    "species.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To select multiple columns, you can pass a list of column names. This will return a DataFrame with the selected columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting multiple columns\n",
    "subset = penguins[['species', 'island', 'bill_length_mm']]\n",
    "subset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "\n",
    "## ⭐️⭐️⭐️💥 What you learned in this session: Three stars and a wish.\n",
    "**In your own words** write in the Markdown cell below:\n",
    "\n",
    "- 3 things you would like to remember from this notebook.\n",
    "- 1 thing you wish to understand better in the future or a question you'd like to ask."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Add your reflections here.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------\n",
    "\n",
    "## Topic Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the Palmer Penguins dataset\n",
    "file_path = 'data/palmer_penguins.csv'\n",
    "penguins = pd.read_csv(file_path)\n",
    "penguins.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Viewing data types of each column\n",
    "penguins.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a small DataFrame from scratch\n",
    "new_data = {\n",
    "    'species': ['Adelie', 'Chinstrap', 'Gentoo'],\n",
    "    'island': ['Torgersen', 'Dream', 'Biscoe'],\n",
    "    'bill_length_mm': [39.1, 48.7, 50.0],\n",
    "    'bill_depth_mm': [18.7, 17.4, 15.3],\n",
    "    'flipper_length_mm': [181, 195, 210],\n",
    "    'body_mass_g': [3750, 3800, 5000],\n",
    "    'sex': ['male', 'female', 'male'],\n",
    "    'year': [2007, 2008, 2009]\n",
    "}\n",
    "\n",
    "new_penguins = pd.DataFrame(new_data)\n",
    "new_penguins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenating the new DataFrame with a subset of the Palmer Penguins dataset\n",
    "subset_penguins = penguins.head(3)  # Taking a subset of the first 3 rows\n",
    "combined_penguins = pd.concat([subset_penguins, new_penguins], ignore_index=True)\n",
    "combined_penguins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting a single column\n",
    "species = penguins['species']\n",
    "species.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------\n",
    "\n",
    "# ⛏ Exercise: Print the penguins.\n",
    "\n",
    "Write a function print_records to print each record from the Palmer Penguins dataset row by row. Use a loop to iterate through each row and print the records in a formatted string.\n",
    "\n",
    "Instructions:\n",
    "\n",
    "1. Load the Palmer Penguins dataset.\n",
    "2. Define a function print_records that takes a DataFrame as an argument.\n",
    "3. Inside the function, use a loop to iterate through each row of the DataFrame.\n",
    "4. Print each record in a formatted string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try to solve the task here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ⛏ Exercise: Formatting the Year Using the datetime Package\n",
    "\n",
    "Write a function format_year to format the year column using the datetime package. Convert the year to include January 1st of each year and add it as a new column formatted_year.\n",
    "\n",
    "Instructions:\n",
    "\n",
    "1. Load the Palmer Penguins dataset.\n",
    "2. Convert the year column to a string if it is not already.\n",
    "3. Define a function format_year that takes a DataFrame as an argument.\n",
    "4. Inside the function, use the datetime package to format the year column to include a specific date (e.g., January 1st of each year).\n",
    "5. Add a new column formatted_year with the formatted date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try to solve the task here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

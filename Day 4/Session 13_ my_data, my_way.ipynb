{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CDCS Summer School\n",
    "# A Gentle Introduction to Coding for Data Analysis\n",
    "## Session 13: my_data, my_way"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------\n",
    "\n",
    "### Learning objectives for this session:\n",
    "\n",
    "At the end of this notebook you will know:\n",
    "\n",
    "1. How to handle missing data.\n",
    "2. Basic string cleaning operations.\n",
    "3. How to detect duplicate data.\n",
    "4. Filtering and subsetting data to look at."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------\n",
    "\n",
    "## 1. Dealing with missing data.\n",
    "\n",
    "Missing data is a common issue in datasets and can occur for various reasons. It's important to identify and handle missing data appropriately to ensure accurate data analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   rowid species     island  bill_length_mm  bill_depth_mm  flipper_length_mm  \\\n",
      "0      1  Adelie  Torgersen            39.1           18.7              181.0   \n",
      "1      2  Adelie  Torgersen            39.5           17.4              186.0   \n",
      "2      3  Adelie  Torgersen            40.3           18.0              195.0   \n",
      "3      4  Adelie  Torgersen             NaN            NaN                NaN   \n",
      "4      5  Adelie  Torgersen            36.7           19.3              193.0   \n",
      "\n",
      "   body_mass_g     sex  year  \n",
      "0       3750.0    male  2007  \n",
      "1       3800.0  female  2007  \n",
      "2       3250.0  female  2007  \n",
      "3          NaN     NaN  2007  \n",
      "4       3450.0  female  2007  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the Palmer Penguins dataset\n",
    "data = '../data/palmer_penguins.csv'\n",
    "penguins = pd.read_csv(data)\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "print(penguins.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before handling missing data, we need to identify where it occurs. Pandas provides several methods to detect missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values in the dataset\n",
    "missing_values_count = penguins.isnull().sum()\n",
    "\n",
    "# Display the count of missing values for each column\n",
    "print(missing_values_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `isnull()` method returns a DataFrame of the same shape as the original, but with boolean values indicating the presence of missing data. By chaining `sum()` method, we count the number of missing values in each column. This gives us a clear overview of where data is missing.\n",
    "\n",
    "One approach to handle missing data is to remove rows or columns with missing values. This method is simple but may result in losing valuable data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows with any missing values\n",
    "penguins_dropped_rows = penguins.dropna()\n",
    "\n",
    "# Remove columns with any missing values\n",
    "penguins_dropped_columns = penguins.dropna(axis=1)\n",
    "\n",
    "# Display the shape of the original and cleaned datasets\n",
    "print('Original dataset shape:', penguins.shape)\n",
    "print('After dropping rows:', penguins_dropped_rows.shape)\n",
    "print('After dropping columns:', penguins_dropped_columns.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `dropna()` method is used to remove rows or columns with missing values. By default, it removes rows, but setting `axis=1` removes columns instead. We compare the shapes of the original and cleaned datasets to understand the impact of removing missing data. This helps us evaluate how much data we lose when we remove rows or columns with missing values.\n",
    "\n",
    "Another approach is to fill in missing values with a specific value or a computed statistic (such as mean, median, or mode). This method retains all data points but introduces potential biases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing values with the mean of the column\n",
    "penguins_filled_mean = penguins.fillna(penguins.mean(numeric_only=True))\n",
    "\n",
    "# Fill missing values with the median of the column\n",
    "penguins_filled_median = penguins.fillna(penguins.median(numeric_only=True))\n",
    "\n",
    "# Fill missing values with the most frequent value (mode) of the column\n",
    "penguins_filled_mode = penguins.apply(lambda x: x.fillna(x.mode()[0]), axis=0)\n",
    "\n",
    "# Display the first few rows of the dataset with filled values\n",
    "print(penguins_filled_mean.head())\n",
    "print(penguins_filled_median.head())\n",
    "print(penguins_filled_mode.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the `fillna()` method to fill missing values. The mean and median are computed using pandas' `mean()` and `median()` methods. For the mode, we apply a lambda function to each column, filling missing values with the most frequent value using `mode()`. Each method of imputation has its pros and cons, and the choice depends on the nature of the data and the analysis requirements.\n",
    "\n",
    "By understanding and applying these techniques, we can effectively handle missing data in the Palmer Penguins dataset. Identifying missing data helps us choose the appropriate method for handling it, whether by removing affected rows/columns or imputing values. Each approach has trade-offs, and the best method depends on the specific dataset and analysis requirements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------\n",
    "\n",
    "## 2. How do I clean text data?\n",
    "\n",
    "String data often requires cleaning to ensure consistency and accuracy. Common string cleaning tasks include removing whitespace, converting case, replacing substrings, and handling missing values.\n",
    "\n",
    "Whitespace at the beginning or end of strings can cause issues in data analysis. We can remove this extra whitespace using the str.strip() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove whitespace from string columns\n",
    "penguins['species'] = penguins['species'].str.strip()\n",
    "penguins['island'] = penguins['island'].str.strip()\n",
    "\n",
    "# Display the first few rows to confirm changes\n",
    "print(penguins[['species', 'island']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the `str.strip()` method to remove leading and trailing whitespace from the 'species' and 'island' columns. This ensures that our string data is clean and consistent, preventing issues during analysis.\n",
    "\n",
    "String data may need to be in a consistent case (all lowercase or all uppercase) for uniformity. We can use the `str.lower()` and `str.upper()` methods to achieve this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert strings to lowercase\n",
    "penguins['species'] = penguins['species'].str.lower()\n",
    "penguins['island'] = penguins['island'].str.lower()\n",
    "\n",
    "# Display the first few rows to confirm changes\n",
    "print(penguins[['species', 'island']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `str.lower()` method converts all characters in the 'species' and 'island' columns to lowercase. This standardizes the text data, making it easier to work with, especially for comparison operations.\n",
    "\n",
    "Sometimes we need to replace specific substrings within our string data. The str.replace() method allows us to do this efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace 'torgersen' with 'torg' in the island column\n",
    "penguins['island'] = penguins['island'].str.replace('torgersen', 'torg')\n",
    "\n",
    "# Display the first few rows to confirm changes\n",
    "print(penguins[['island']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the `str.replace()` method to replace occurrences of 'torgersen' with 'torg' in the 'island' column. This can be useful for shortening names or standardising variations of substrings.\n",
    "\n",
    "Just like numeric data, string data can also have missing values that need to be handled. We can fill missing values using the `fillna()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing values in string columns with a placeholder\n",
    "penguins['species'] = penguins['species'].fillna('unknown')\n",
    "penguins['island'] = penguins['island'].fillna('unknown')\n",
    "\n",
    "# Display the count of missing values to confirm changes\n",
    "print(penguins.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the `fillna()` method to replace missing values in the 'species' and 'island' columns with the placeholder 'unknown'. This ensures that there are no missing values in these columns, making the dataset more robust for analysis.\n",
    "\n",
    "Sometimes, we need to combine multiple string columns into one. The str.cat() method allows us to concatenate strings from different columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine species and island columns into a new column\n",
    "penguins['species_island'] = penguins['species'].str.cat(penguins['island'], sep=' - ')\n",
    "\n",
    "# Display the first few rows to confirm changes\n",
    "print(penguins[['species', 'island', 'species_island']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `str.cat()` method concatenates the 'species' and 'island' columns with a separator ' - '. This creates a new column 'species_island' that combines information from both columns, which can be useful for creating unique identifiers or descriptive labels.\n",
    "\n",
    "We can extract specific parts of strings using the `str.extract()` method, which is particularly useful for pattern matching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Extract the first word from the species column\n",
    "penguins['species_first_word'] = penguins['species'].str.extract(r'(\\w+)')\n",
    "\n",
    "# Display the first few rows to confirm changes\n",
    "print(penguins[['species', 'species_first_word']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the `str.extract()` method with a regular expression, we extract the first word from the 'species' column. This creates a new column 'species_first_word', which can be useful for simplifying or categorising text data.\n",
    "\n",
    "We may need to split a single string column into multiple columns. The str.split() method helps us achieve this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the species_island column back into two separate columns\n",
    "penguins[['species_split', 'island_split']] = penguins['species_island'].str.split(' - ', expand=True)\n",
    "\n",
    "# Display the first few rows to confirm changes\n",
    "print(penguins[['species_island', 'species_split', 'island_split']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `str.split()` method splits the 'species_island' column into two separate columns 'species_split' and 'island_split' based on the separator ' - '. This demonstrates how we can reverse the process of concatenation or handle columns containing combined information.\n",
    "\n",
    "By understanding and applying these string cleaning operations, we can ensure that the text data in the Palmer Penguins dataset is consistent and ready for analysis. Removing whitespace, standardising case, replacing substrings, handling missing values, combining, extracting, and splitting columns are essential techniques for effective string manipulation in pandas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------\n",
    "\n",
    "## 3. Dealing with duplicates.\n",
    "\n",
    "Duplicate data can cause inaccuracies in analysis and results. It is important to identify and handle duplicates to ensure data integrity. Duplicates can occur in both rows and columns, and handling them involves either removing or aggregating the duplicate entries.\n",
    "\n",
    "Identifying duplicate rows is the first step in handling duplicate data. We can use the `duplicated()` method to find duplicate rows in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicate rows\n",
    "duplicate_rows = penguins.duplicated()\n",
    "\n",
    "# Display the count of duplicate rows\n",
    "print(f'Total duplicate rows: {duplicate_rows.sum()}')\n",
    "\n",
    "# Show the duplicate rows\n",
    "print(penguins[duplicate_rows])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `duplicated()` method returns a boolean Series indicating whether each row is a duplicate. We sum this Series to get the total number of duplicate rows and display the actual duplicate rows for inspection.\n",
    "\n",
    "Once duplicates are identified, we can remove them using the `drop_duplicates()` method. This method removes duplicate rows, keeping the first occurrence by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicate rows\n",
    "penguins_no_duplicates = penguins.drop_duplicates()\n",
    "\n",
    "# Display the shape of the original and cleaned datasets\n",
    "print('Original dataset shape:', penguins.shape)\n",
    "print('After removing duplicates:', penguins_no_duplicates.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `drop_duplicates()` method removes duplicate rows from the dataset. We compare the shapes of the original and cleaned datasets to understand the impact of removing duplicates.\n",
    "\n",
    "Duplicate columns can also occur, especially when merging datasets. We can identify duplicate columns by comparing the values across columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicate columns by transposing the dataframe and using duplicated\n",
    "duplicate_columns = penguins.T.duplicated()\n",
    "\n",
    "# Display the duplicate columns\n",
    "print(f'Duplicate columns: {duplicate_columns.sum()}')\n",
    "print(penguins.T[duplicate_columns].T.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We transpose the DataFrame and use the `duplicated()` method to identify duplicate columns. Transposing the DataFrame switches rows and columns, allowing us to use the same method for detecting duplicates.\n",
    "\n",
    "After identifying duplicate columns, we can remove them by selecting only the unique columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicate columns by transposing the dataframe, removing duplicates, and transposing back\n",
    "penguins_no_duplicate_columns = penguins.loc[:, ~penguins.T.duplicated()]\n",
    "\n",
    "# Display the shape of the original and cleaned datasets\n",
    "print('Original dataset shape:', penguins.shape)\n",
    "print('After removing duplicate columns:', penguins_no_duplicate_columns.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We remove duplicate columns by transposing the DataFrame, removing the duplicate columns, and transposing it back. This ensures that we retain only unique columns in the dataset.\n",
    "\n",
    "Sometimes, rows may not be exact duplicates but may still need to be handled due to partial duplication. We can use the `drop_duplicates()` method with specific columns to handle this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove partial duplicates based on selected columns\n",
    "penguins_partial_no_duplicates = penguins.drop_duplicates(subset=['species', 'island'])\n",
    "\n",
    "# Display the shape of the original and cleaned datasets\n",
    "print('Original dataset shape:', penguins.shape)\n",
    "print('After removing partial duplicates:', penguins_partial_no_duplicates.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `drop_duplicates()` method can be used with the `subset` parameter to specify which columns to consider when identifying duplicates. This allows us to handle partial duplicates based on specific columns.\n",
    "\n",
    "In some cases, instead of removing duplicates, we might want to aggregate the duplicate entries. We can use the `groupby()` method to achieve this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate duplicate rows by species and island, calculating the mean for numeric columns\n",
    "penguins_aggregated = penguins.groupby(['species', 'island']).mean().reset_index()\n",
    "\n",
    "# Display the first few rows of the aggregated dataset\n",
    "print(penguins_aggregated.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the `groupby()` method to group the data by the 'species' and 'island' columns and calculate the mean for the numeric columns. This aggregates the duplicate entries, providing a summary for each group.\n",
    "\n",
    "It's often useful to combine duplicate detection and cleaning in a single process to streamline the workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect and remove duplicate rows, then handle partial duplicates and aggregate\n",
    "penguins_cleaned = (penguins\n",
    "                    .drop_duplicates()\n",
    "                    .drop_duplicates(subset=['species', 'island'])\n",
    "                    .groupby(['species', 'island']).mean().reset_index())\n",
    "\n",
    "# Display the first few rows of the cleaned dataset\n",
    "print(penguins_cleaned.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We combine the steps of detecting and removing duplicate rows, handling partial duplicates, and aggregating the data. This creates a clean dataset ready for analysis.\n",
    "\n",
    "By understanding and applying these techniques, we can effectively detect and handle duplicate data in the Palmer Penguins dataset. Identifying duplicates, removing duplicate rows and columns, handling partial duplicates, and aggregating duplicate entries are essential techniques for ensuring data integrity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------\n",
    "\n",
    "## 4. Filtering and subsetting data.\n",
    "\n",
    "Filtering and subsetting data are fundamental operations in data analysis. They allow us to focus on specific parts of a dataset, making it easier to analyze and derive insights. These operations can be based on conditions, specific columns, or rows.\n",
    "\n",
    "Filtering rows based on conditions allows us to select specific data points that meet certain criteria. This is useful for focusing on relevant subsets of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter rows where species is 'adelie'\n",
    "adelie_penguins = penguins[penguins['species'] == 'adelie']\n",
    "\n",
    "# Display the first few rows of the filtered dataset\n",
    "print(adelie_penguins.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We filter the rows where the 'species' column is equal to 'Adelie'. This creates a new DataFrame containing only Adelie penguins. Filtering based on conditions helps narrow down the dataset to specific observations.\n",
    "\n",
    "Sometimes, we need to filter rows based on multiple conditions. We can use logical operators to combine these conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter rows where species is 'Adelie' and island is 'Torgersen'\n",
    "adelie_torgersen_penguins = penguins[(penguins['species'] == 'adelie') & (penguins['island'] == 'torg')]\n",
    "\n",
    "# Display the first few rows of the filtered dataset\n",
    "print(adelie_torgersen_penguins.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We filter the rows where the 'species' is 'Adelie' and the 'island' is 'Torgersen' using the & operator. This creates a new DataFrame containing only the Adelie penguins found on Torgersen island. Combining conditions allows for more refined filtering.\n",
    "\n",
    "More complex conditions can be handled using the `query()` method, which provides a more readable way to specify conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter rows where body_mass_g is greater than 4000 and species is not 'Adelie'\n",
    "large_penguins = penguins.query('body_mass_g > 4000 and species != \"adelie\"')\n",
    "\n",
    "# Display the first few rows of the filtered dataset\n",
    "print(large_penguins.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `query()` method allows us to filter rows using a query string. In this example, we filter rows where the body mass is greater than 4000 grams and the species is not 'Adelie'. This method makes the code more readable, especially for complex conditions.\n",
    "\n",
    "Subsetting columns allows us to select specific columns from the dataset. This is useful when we are only interested in certain variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select specific columns: species, island, and body_mass_g\n",
    "selected_columns = penguins[['species', 'island', 'body_mass_g']]\n",
    "\n",
    "# Display the first few rows of the subsetted dataset\n",
    "print(selected_columns.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use double square brackets to select specific columns from the DataFrame. This creates a new DataFrame containing only the 'species', 'island', and 'body_mass_g' columns. Subsetting columns helps reduce the dataset to relevant variables.\n",
    "\n",
    "We can combine row and column filtering to create more specific subsets of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter rows and select specific columns\n",
    "adelie_mass = penguins[penguins['species'] == 'adelie'][['island', 'body_mass_g']]\n",
    "\n",
    "# Display the first few rows of the filtered and subsetted dataset\n",
    "print(adelie_mass.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first filter the rows where the 'species' is 'Adelie' and then select the 'island' and 'body_mass_g' columns. This creates a DataFrame with only the relevant rows and columns, focusing on the body mass of Adelie penguins on different islands.\n",
    "\n",
    "Boolean indexing is a powerful technique for subsetting data based on boolean conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a boolean mask for penguins with flipper_length_mm greater than 200\n",
    "mask = penguins['flipper_length_mm'] > 200\n",
    "\n",
    "# Use the mask to subset the dataset\n",
    "long_flipper_penguins = penguins[mask]\n",
    "\n",
    "# Display the first few rows of the subsetted dataset\n",
    "print(long_flipper_penguins.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a boolean mask where the 'flipper_length_mm' is greater than 200. Using this mask, we subset the dataset to include only the rows where the condition is true. Boolean indexing provides a flexible way to filter data based on various conditions.\n",
    "\n",
    "The `loc` method allows for conditional subsetting using labels and boolean conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use loc to filter rows and select columns\n",
    "long_flipper_adelie = penguins.loc[(penguins['flipper_length_mm'] > 200) & (penguins['species'] == 'adelie'), ['island', 'body_mass_g']]\n",
    "\n",
    "# Display the first few rows of the filtered and subsetted dataset\n",
    "print(long_flipper_adelie.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the `loc` method to filter rows based on the condition that the 'flipper_length_mm' is greater than 200 and the 'species' is 'Adelie'. Additionally, we select the 'island' and 'body_mass_g' columns. The `loc` method provides a concise way to subset data based on both row conditions and column selection.\n",
    "\n",
    "By understanding and applying these filtering and subsetting techniques, we can focus on specific parts of the Palmer Penguins dataset that are relevant to our analysis. Filtering rows based on conditions, subsetting columns, and combining these operations help create targeted datasets for deeper analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "\n",
    "## ‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏èüí• What you learned in this session: Three stars and a wish.\n",
    "**In your own words** write in the Markdown cell below:\n",
    "\n",
    "- 3 things you would like to remember from this notebook.\n",
    "- 1 thing you wish to understand better in the future or a question you'd like to ask."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Add your reflections here.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------\n",
    "\n",
    "## Topic Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify missing values\n",
    "print(\"Missing values in each column:\")\n",
    "print(penguins.isnull().sum())\n",
    "\n",
    "# Fill missing values with the mean for numeric columns\n",
    "penguins_filled = penguins.fillna(penguins.mean(numeric_only=True))\n",
    "\n",
    "# Verify that missing values have been handled\n",
    "print(\"\\nMissing values after filling with mean:\")\n",
    "print(penguins_filled.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove whitespace from 'species' and 'island' columns\n",
    "penguins['species'] = penguins['species'].str.strip()\n",
    "penguins['island'] = penguins['island'].str.strip()\n",
    "\n",
    "# Convert 'species' and 'island' columns to lowercase\n",
    "penguins['species'] = penguins['species'].str.lower()\n",
    "penguins['island'] = penguins['island'].str.lower()\n",
    "\n",
    "# Replace 'torgersen' with 'torg' in the 'island' column\n",
    "penguins['island'] = penguins['island'].str.replace('torgersen', 'torg')\n",
    "\n",
    "# Display cleaned data\n",
    "print(penguins[['species', 'island']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicate rows\n",
    "duplicate_rows = penguins.duplicated()\n",
    "print(f\"Total duplicate rows: {duplicate_rows.sum()}\")\n",
    "\n",
    "# Remove duplicate rows\n",
    "penguins_no_duplicates = penguins.drop_duplicates()\n",
    "\n",
    "# Verify that duplicates have been removed\n",
    "print(\"Dataset shape after removing duplicates:\")\n",
    "print(penguins_no_duplicates.shape)\n",
    "\n",
    "# Check for duplicate columns by transposing the dataframe\n",
    "duplicate_columns = penguins.T.duplicated()\n",
    "print(f\"Total duplicate columns: {duplicate_columns.sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter rows where species is 'Adelie'\n",
    "adelie_penguins = penguins[penguins['species'] == 'adelie']\n",
    "\n",
    "# Filter rows where body_mass_g is greater than 4000 and species is not 'Adelie'\n",
    "large_penguins = penguins.query('body_mass_g > 4000 and species != \"adelie\"')\n",
    "\n",
    "# Select specific columns: 'species', 'island', and 'body_mass_g'\n",
    "selected_columns = penguins[['species', 'island', 'body_mass_g']]\n",
    "\n",
    "# Combine filtering and column selection\n",
    "adelie_mass = penguins[penguins['species'] == 'adelie'][['island', 'body_mass_g']]\n",
    "\n",
    "# Display the filtered and subsetted data\n",
    "print(adelie_mass.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------\n",
    "\n",
    "# ‚õè Exercise: Clean up the penguins!\n",
    "\n",
    "In this exercise, you will work with the Palmer Penguins dataset to handle missing data and perform advanced string cleaning operations. You will need to identify and impute missing values, and clean string data to ensure consistency.\n",
    "\n",
    "### Instructions:\n",
    "\n",
    "1. Load the Palmer Penguins dataset.\n",
    "2. Identify columns with missing values and impute these missing values using the median for numeric columns and the most frequent value (mode) for categorical columns.\n",
    "3. Clean the 'species' and 'island' columns by:\n",
    "    - Removing any leading and trailing whitespace.\n",
    "    - Converting all text to lowercase.\n",
    "    - Replacing 'dream' with 'drm' and 'torgersen' with 'torg'.\n",
    "4. Create a new column 'species_island' that concatenates the 'species' and 'island' columns with a separator ' - '."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try to solve the task here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ‚õè Exercise: Select the right penguins!\n",
    "\n",
    "In this exercise, you will detect and handle duplicate data in the Palmer Penguins dataset. You will also perform complex filtering operations to create subsets of the data based on multiple conditions.\n",
    "\n",
    "Instructions:\n",
    "\n",
    "1. Load the Palmer Penguins dataset.\n",
    "2. Detect and remove any duplicate rows.\n",
    "3. Check for duplicate columns and remove them if any.\n",
    "4. Filter the dataset to create the following subsets:\n",
    "    - All penguins with a flipper length greater than 200 mm.\n",
    "    - All Adelie penguins with a body mass greater than 4000 grams, showing only the 'species', 'island', and 'body_mass_g' columns.\n",
    "5. Combine the above filtering operations to create a final subset showing only Adelie penguins with a flipper length greater than 200 mm and a body mass greater than 4000 grams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try to solve the task here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
